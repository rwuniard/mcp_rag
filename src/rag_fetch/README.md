# RAG Fetch - Document Search & Retrieval Service

A professional semantic search and retrieval service for RAG (Retrieval-Augmented Generation) systems. Provides fast similarity search across vector-embedded documents with MCP (Model Context Protocol) integration for AI assistants.

## üéØ Purpose

RAG Fetch is the **search microservice** that:
- Performs semantic similarity search across stored documents
- Returns MCP-compatible JSON responses for AI assistants
- Provides both CLI and MCP server interfaces
- Connects to ChromaDB server via HTTP client

## üöÄ Quick Start

### 1. Environment Setup

Create a `.env` file in this directory:
```bash
# Required for Google embeddings (default)
GOOGLE_API_KEY=your_google_api_key_here

# Optional for OpenAI embeddings
OPENAI_API_KEY=your_openai_api_key_here

# ChromaDB Server Configuration
CHROMADB_HOST=localhost
CHROMADB_PORT=8000
CHROMADB_COLLECTION_NAME=langchain
```

### 2. Start ChromaDB Server

Start the ChromaDB server before searching:
```bash
# Start ChromaDB server (from project root)
./scripts/chromadb-server.sh start

# Check server health
./scripts/chromadb-server.sh health
```

### 3. Ensure Documents are Indexed

Make sure you have documents stored by the RAG Store service:
```bash
# Store documents to ChromaDB server
python main.py store
```

### 4. Search Documents

```bash
# From project root - CLI search
python main.py search "machine learning concepts"

# Or directly
python src/rag_fetch/cli.py "artificial intelligence"

# Or using the CLI command (if installed)
rag-fetch-cli "interesting facts about animals"

# Start MCP server for AI assistants
python main.py server
# Or: rag-mcp-server
```

## üìÅ Project Structure

```
src/rag_fetch/
‚îú‚îÄ‚îÄ README.md                 # This file
‚îú‚îÄ‚îÄ .env                     # Environment variables
‚îú‚îÄ‚îÄ search_similarity.py     # Core search functionality
‚îú‚îÄ‚îÄ mcp_server.py           # MCP server implementation
‚îî‚îÄ‚îÄ cli.py                  # Command-line interface
```

## üîß Components

### **Search Similarity** (`search_similarity.py`)
- **Purpose**: Core semantic search functionality with real-time data access
- **Technology**: ChromaDB HTTP Client + LangChain + Google/OpenAI embeddings
- **Features**: Similarity search with scores, MCP-compatible JSON responses, cached connections
- **Models**: Google `text-embedding-004` (default), OpenAI `text-embedding-ada-002`
- **Architecture**: Client-server with persistent HTTP connections to ChromaDB

### **MCP Server** (`mcp_server.py`)
- **Purpose**: Model Context Protocol server for AI assistant integration
- **Framework**: FastMCP
- **Tools**: `search_documents(query, limit)` 
- **Response**: Structured JSON with content, metadata, and relevance scores
- **Real-time Data**: Server architecture ensures immediate access to new documents

### **CLI Interface** (`cli.py`)
- **Purpose**: Command-line search interface
- **Usage**: `rag-fetch-cli "search query"`
- **Features**: Human-friendly output, error handling

## üîÑ Real-time Data Access

### **Problem Solved**
Previously, when new documents were added to ChromaDB via RAG Store, the MCP server wouldn't see them until manually restarted due to file-based ChromaDB internal caching.

### **Solution Architecture**
The client-server architecture provides **real-time data freshness** with three key components:

#### **1. ChromaDB HTTP Client**
```python
def get_chromadb_client() -> chromadb.Client:
    """Connect to ChromaDB server via HTTP."""
    client = chromadb.HttpClient(host=CHROMADB_HOST, port=CHROMADB_PORT)
    client.heartbeat()  # Test connection
    return client
```

#### **2. Cached HTTP Connections**
```python
def get_cached_vectorstore(model_vendor: ModelVendor, collection_name: str = None):
    """Get vectorstore with cached HTTP connections."""
    # 30-second TTL for connection performance
    # Server ensures data freshness automatically
    # Cached connections per model/collection
```

#### **3. Server-Guaranteed Freshness**
```python
def similarity_search_mcp_tool(query: str, ...):
    """MCP tool with server-guaranteed fresh data."""
    # Uses cached HTTP connection to ChromaDB server
    vectorstore = get_cached_vectorstore(model_vendor, collection)
    # Server is single source of truth for data
```

### **Performance Characteristics**
- **HTTP Client**: Persistent connections with minimal overhead
- **Connection Caching**: 30-second TTL reduces connection establishment costs
- **Memory Impact**: Minimal (single cached connection per model/collection)
- **Error Handling**: Clear error messages if server unavailable

### **Benefits**
- ‚úÖ **Zero Downtime**: No MCP server restarts needed
- ‚úÖ **Always Fresh**: Server ensures all clients see latest data immediately
- ‚úÖ **Server Architecture**: Centralized ChromaDB server with HTTP API
- ‚úÖ **Docker Integration**: Easy server management with Docker Compose
- ‚úÖ **Production Ready**: Comprehensive error handling and monitoring

## üîç Search Functionality

### **Basic Similarity Search**
```python
from rag_fetch.search_similarity import similarity_search_mcp_tool, ModelVendor

# Search with Google embeddings (default)
results = similarity_search_mcp_tool(
    query="machine learning algorithms",
    model_vendor=ModelVendor.GOOGLE,
    limit=6
)
```

### **MCP Tool Integration**
```python
# Used by AI assistants via MCP
search_documents(
    query="What are the privacy laws in California?",
    limit=6
)
```

### **Response Format**
```json
{
  "query": "machine learning algorithms",
  "results": [
    {
      "content": "Machine learning is a subset of artificial intelligence...",
      "metadata": {
        "source": "ai_handbook.pdf",
        "chunk_id": "chunk_0",
        "document_id": "ai_handbook_pdf",
        "page": 15
      },
      "relevance_score": 0.89
    }
  ],
  "total_results": 3,
  "status": "success"
}
```

## ü§ñ MCP Server Integration

### **Available Tools**

| Tool | Description | Parameters | Returns |
|------|-------------|------------|---------|
| `search_documents` | Semantic document search | `query` (string), `limit` (int, default: 6) | JSON with results, metadata, scores |

### **Adding to Claude Desktop**

Add to `~/Library/Application Support/Claude/claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "rag-knowledge-base": {
      "command": "/usr/local/bin/uv",
      "args": [
        "--directory", "/Users/YOUR_USERNAME/Projects/python/mcp_rag",
        "run", "python", "src/rag_fetch/mcp_server.py"
      ]
    }
  }
}
```

### **Adding to Cursor**

Add to Cursor MCP configuration:

```json
{
  "mcpServers": {
    "rag-knowledge-base": {
      "command": "/usr/local/bin/uv",
      "args": [
        "--directory", "/absolute/path/to/mcp_rag",
        "run", "python", "src/rag_fetch/mcp_server.py"
      ]
    }
  }
}
```

## üéõÔ∏è Configuration

### **Environment Variables**
```bash
# Google AI (default)
GOOGLE_API_KEY=your_google_api_key

# OpenAI (alternative)
OPENAI_API_KEY=your_openai_api_key

# ChromaDB Server
CHROMADB_HOST=localhost
CHROMADB_PORT=8000
```

### **Model Selection**
```python
from rag_fetch.search_similarity import ModelVendor

# Use Google embeddings (default)
ModelVendor.GOOGLE

# Use OpenAI embeddings
ModelVendor.OPENAI
```

### **ChromaDB Server**
- **Server URL**: `http://localhost:8000`
- **Data Storage**: `../../../data/chroma_data/`
- **Management**: `./scripts/chromadb-server.sh`

## üîß API Reference

### **Core Functions**

```python
# Load vector database
vectorstore = load_vectorstore(ModelVendor.GOOGLE)

# Basic similarity search
documents = search_similarity(query, vectorstore, k=6)

# Search with JSON response
json_result = search_similarity_with_json_result(query, vectorstore, number_result=6)

# MCP tool wrapper (with auto-refresh)
mcp_response = similarity_search_mcp_tool(query, ModelVendor.GOOGLE, limit=6)

# Auto-refresh functions (new in latest version)
success = refresh_vectorstore_data(vectorstore)
cached_vectorstore = get_cached_vectorstore(ModelVendor.GOOGLE)
```

### **Search Parameters**

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `query` | string | required | Natural language search query |
| `model_vendor` | ModelVendor | GOOGLE | Embedding model to use |
| `limit` | int | 6 | Maximum number of results |
| `collection` | string | None | Specific collection to search |

## üîç Usage Examples

### **CLI Search**
```bash
# Simple search
rag-fetch-cli "What is Python?"

# Search from any directory
python src/rag_fetch/cli.py "machine learning concepts"
```

### **MCP Server**
```bash
# Start MCP server
rag-mcp-server

# Server provides 'search_documents' tool to AI assistants
```

### **Programmatic Usage**
```python
from rag_fetch.search_similarity import similarity_search_mcp_tool, ModelVendor
import json

# Perform search
result_json = similarity_search_mcp_tool(
    "California privacy laws",
    ModelVendor.GOOGLE,
    limit=5
)

# Parse results
result = json.loads(result_json)
for doc in result["results"]:
    print(f"Content: {doc['content'][:100]}...")
    print(f"Source: {doc['metadata']['source']}")
    print(f"Score: {doc['relevance_score']}")
```

## üèóÔ∏è Architecture Integration

RAG Fetch works as an **independent search microservice**:

- **Input**: Search queries via CLI, MCP, or API
- **Data Source**: ChromaDB server (populated by RAG Store)
- **Output**: Structured JSON responses with content and metadata

**Microservices Flow**:
```
AI Assistant ‚Üí MCP Server ‚Üí RAG Fetch ‚Üí ChromaDB Server ‚Üí Search Results
     ‚Üì
User Query ‚Üí CLI ‚Üí RAG Fetch ‚Üí ChromaDB Server ‚Üí Human-friendly Output
```

## üß™ Quality Features

### **Advanced Search**
- **Semantic Search**: Uses vector embeddings for meaning-based matching
- **Relevance Scoring**: Returns similarity scores for result ranking
- **Multiple Models**: Support for Google and OpenAI embedding models
- **Error Handling**: Graceful degradation with meaningful error messages

### **MCP Compatibility**
- **Structured JSON**: Standardized response format for AI tools
- **Metadata Rich**: Includes source files, page numbers, chunk IDs
- **Tool Discovery**: Proper MCP tool registration and documentation

### **Performance**
- **Fast Retrieval**: Optimized vector similarity search
- **Configurable Limits**: Adjustable result count for different use cases
- **Efficient Loading**: Smart database connection management

## üìä Performance Metrics

### **Search Speed**
- **Typical Query**: < 200ms for 6 results
- **Large Database**: Scales with ChromaDB performance
- **Memory Usage**: < 100MB for typical workloads

### **Accuracy**
- **Embedding Quality**: Leverages Google's `text-embedding-004` model
- **Relevance Scores**: 0.0-1.0 scale for result ranking
- **Search Quality**: Optimized chunking from RAG Store improves relevance

## üîó Integration Examples

### **AI Assistant Usage**
```
User: "Can you find information about machine learning algorithms?"
AI Assistant: [Uses search_documents tool via MCP]
AI: "I found several relevant documents about machine learning algorithms..."
```

### **Development Workflow**
```bash
# 1. Store documents
python main.py store

# 2. Test search
python main.py search "test query"

# 3. Start MCP server for AI integration
python main.py server
```

## üß™ Development

### **Testing**
```bash
# Test search functionality
python src/rag_fetch/search_similarity.py

# Run fetch-specific tests
python -m unittest tests.test_rag_fetch.test_search_similarity -v
```

### **Dependencies**
```toml
# Core dependencies
chromadb = ">=1.0.17"
fastmcp = ">=2.11.3"
langchain = ">=0.3.27"
langchain-chroma = ">=0.2.5"
langchain-google-genai = ">=2.0.10"
langchain-openai = ">=0.3.30"
python-dotenv = ">=1.1.1"
```

## üîó Related Services

- **RAG Store**: Document ingestion and vector storage service
- **Main CLI**: Unified command interface for both services
- **MCP Integration**: Model Context Protocol server for AI assistants

## üõ†Ô∏è Troubleshooting

### **Common Issues**

**"Cannot connect to ChromaDB server" error**:
```bash
# Solution: Start ChromaDB server first
./scripts/chromadb-server.sh start

# Check server health
./scripts/chromadb-server.sh health
```

**"API key required" error**:
```bash
# Solution: Add API key to .env file
echo "GOOGLE_API_KEY=your_key_here" >> .env
```

**No search results**:
```bash
# Check if ChromaDB server is running
./scripts/chromadb-server.sh status

# If no documents, run document ingestion
python main.py store
```

### **Server Connection Troubleshooting**

**MCP server not seeing new documents**:
```bash
# Check ChromaDB server status
./scripts/chromadb-server.sh health

# Test search directly
python src/rag_fetch/search_similarity.py
# Should show latest documents immediately
```

**Connection timeout errors**:
```bash
# Check if server is running
./scripts/chromadb-server.sh status

# Check server logs for issues
./scripts/chromadb-server.sh logs

# Restart server if needed
./scripts/chromadb-server.sh restart
```

**Performance issues**:
```bash
# Check connection cache TTL (default 30 seconds)
# Server handles data freshness, cache optimizes connections
```

**Memory usage increasing over time**:
```bash
# Connection cache automatically manages memory:
python -c "from src.rag_fetch.search_similarity import _vectorstore_cache; _vectorstore_cache.clear()"
```

### **Development & Testing**

**Running server-specific tests**:
```bash
# Test server connection
python -c "from src.rag_fetch.search_similarity import get_chromadb_client; client = get_chromadb_client(); print('‚úÖ Connected')"

# Test caching system
python -m pytest tests/test_rag_fetch/test_search_similarity.py::TestCachedVectorstore -v

# Test MCP tool with server
python -m pytest tests/test_rag_fetch/test_search_similarity.py -v
```

**Performance testing**:
```bash
# Benchmark search performance
python -c "
import time
from src.rag_fetch.search_similarity import similarity_search_mcp_tool, ModelVendor

start = time.time()
result = similarity_search_mcp_tool('test query', ModelVendor.GOOGLE, limit=3)
print(f'Search took: {(time.time() - start) * 1000:.2f}ms')
"
```

## üîÑ Version History & Changelog

### **v2.0.0 - Client-Server Architecture** *(Latest)*
- ‚úÖ **Client-Server Architecture**: ChromaDB server with HTTP clients
- ‚úÖ **Real-time Data Access**: Server ensures immediate data freshness
- ‚úÖ **Docker Integration**: Easy ChromaDB server management
- ‚úÖ **Cached Connections**: 30-second TTL for HTTP connection optimization
- ‚úÖ **Server Management**: Complete start/stop/status/health commands
- ‚úÖ **Production Ready**: Robust server-based architecture

### **v1.0.0 - Initial Release**
- ‚úÖ **Core Search Functionality**: Semantic similarity search with ChromaDB
- ‚úÖ **MCP Integration**: Model Context Protocol server for AI assistants
- ‚úÖ **Multi-Model Support**: Google and OpenAI embedding models
- ‚úÖ **CLI Interface**: Command-line search capabilities
- ‚úÖ **Production Ready**: Comprehensive error handling and testing

## üìÑ License

Part of the MCP RAG project - MIT License